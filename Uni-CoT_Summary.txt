这篇工作提出了Uni-CoT框架，通过双层次推理架构实现了文本和视觉模态的统一链式思考推理，显著降低了多模态推理的计算复杂度。
#### 动机
传统的Chain-of-Thought（CoT）推理在纯文本大语言模型上取得了巨大成功，但将其扩展到多模态领域面临重大挑战。现有的多模态推理方法存在两个核心问题：首先，它们难以有效建模动态的视觉状态转换，这在几何问题的图表更新、视觉谜题的迭代编辑等复杂场景中至关重要；其次，现有的模块化框架采用分离的推理和生成组件，导致梯度流断裂，产生碎片化的推理轨迹和视觉上不一致的状态转换。
更进一步，多模态推理与纯文本推理存在本质差异——人类的多模态推理涉及解释动态的视觉状态转换，包括物体运动、空间布局和视觉因果关系，这些是仅通过离散符号表示难以建模的。因此，迫切需要一个能够处理结构化视觉转换的统一框架。
#### 主要贡献
这项工作的核心贡献体现在四个方面：第一，提出了Uni-CoT统一框架，能够在单一模型内实现连贯的多模态推理；第二，设计了创新的双层次推理架构，包括用于高层规划的宏观层次CoT和用于子任务执行的微观层次CoT；第三，显著降低了计算复杂度，使得所有实验都能在8块A100 GPU上高效完成；第四，在多个推理驱动的图像生成和编辑基准测试中达到了最先进的性能表现。
#### 详细的方法机制
基础模型：BAGEL
Uni-CoT建立在BAGEL（Scalable Generative Cognitive Model）基础上，这是一个支持联合视觉-语言理解和生成的统一基础模型。BAGEL采用仅解码器的transformer架构，具有混合transformer专家（Mixture-of-Transformer-Experts）设计。
架构上，BAGEL实例化了两个专家：理解专家和生成专家。两个专家通过统一的自注意力机制在共享的多模态token序列上操作。模型集成了两个模态特定的视觉编码器：用于语义级理解的Vision Transformer（ViT）编码器，将输入图像转换为约4900个token；用于像素级生成的变分自编码器（VAE），将图像编码为64×64的潜在网格，包含16个通道，产生4096个潜在token。
专家路由机制采用硬门控：理解专家处理文本和ViT token，而生成专家专门处理VAE token。训练目标结合了两个互补的损失函数：文本token的交叉熵损失$$L_{CE}^{text} = \sum_{i=1}^{C} x_i \log(\hat{x_i})$$，以及基于修正流范式的图像生成均方误差损失$$L_{MSE}^{image} = E[|g_\theta(x_t | c) - (x_0 - x_1)|^2]$$。
宏观层次CoT：规划策略
宏观层次CoT负责高层任务规划，将复杂任务分解为更简单、可处理的子任务序列。受人类认知行为启发，设计了三种规划机制：
序列分解机制模拟人类以逐步方式处理中间目标的行为，将任务分割为固定的子任务序列。这种结构化进展简化了整体推理路径并提高了可追溯性。
并行分解机制受协作场景启发，任务被分割为可以并发解决的独立组件，使模型能够并行推理多个子任务，利用任务模块性提高效率。
通过渐进式优化的隐式规划机制模拟在不确定环境中的人类行为，模型增量地优化其计划，如果出现不一致性就修订早期步骤，支持自适应和灵活的推理。
为了在训练和推理期间强制实现这种抽象，引入了宏观掩码注意力方案。这种掩码机制选择性地只显示系统提示、宏观层次规划输出和最终子任务结果，完全掩码所有中间推理痕迹。
微观层次CoT：子任务执行
微观层次CoT负责通过迭代、反馈驱动的推理执行由宏观层次规划器分配的子任务。核心设计是自检（Self-Check）机制，模型在尝试完成子任务后评估其输出质量并确定是否需要修订。
关键创新是将自反思过程建模为马尔可夫决策过程（MDP），其中每个自反思步骤只依赖于前一步的结果和给定的子任务指令。MDP的元素定义如下：
状态$$s_t$$表示当前推理状态，包含前一步的输出（文本和视觉内容）；动作$$a_t$$是结合文本编辑提示生成和相应图像编辑的混合操作；下一状态$$s_{t+1}$$是更新的状态，包含编辑后的图像和对齐的文本摘要；奖励$$r_{t+1}$$是衡量下一状态结果与子任务目标对齐程度的文本判断。
为了强制这种局部性，引入了微观掩码注意力方案，将注意力限制在短程状态转换上。微观层次CoT的学习目标分解为两个核心能力：子任务完成（涉及图像编辑和多模态理解）和MDP建模（包括混合动作生成、下一状态预测和奖励估计）。
训练范式
训练分为监督微调（SFT）和强化学习（RL）两个阶段。
监督微调阶段包含两个互补组件：宏观层次CoT的交错多模态监督和微观层次CoT的多任务学习。对于宏观层次，采用结合的损失公式$$L_{Macro} = \lambda_{CE} \cdot L_{CE}^{text} + L_{MSE}^{image}$$，其中$$\lambda_{CE} = 0.25$$确保训练稳定性。
对于微观层次，子任务完成使用交错多模态数据监督，同样采用CE和MSE损失。为支持基于MDP的自反思推理过程建模，将学习分解为四个辅助目标：文本动作生成使用交叉熵损失、图像动作生成使用MSE损失、下一状态预测使用交叉熵损失、奖励估计使用交叉熵损失。
强化学习阶段采用简化而有效的直接偏好优化（DPO）策略，将DPO训练解耦为两个偏好建模阶段：文本偏好学习鼓励模型偏好连贯正确的推理路径；视觉偏好学习基于捕捉语义正确性、视觉保真度和指令对齐的人类或代理反馈指导图像编辑行为。
#### 实验
实验主要聚焦于Uni-CoT框架内的图像生成任务，评估两类推理中心的生成任务：推理驱动的图像生成和推理驱动的图像编辑。
对于图像生成任务，在广泛采用的WISE基准上进行实验，该基准评估模型在复杂推理提示条件下生成连贯忠实视觉输出的能力。对于图像编辑任务，在RISE和KRIS基准上进行评估，两者都针对复杂指令下的视觉推理和编辑。
定量结果显示，Uni-CoT在WISE的所有评估域上一致达到最先进结果，在文化、时间、空间、生物学、物理学、化学等领域均超越开源基线。在KRIS基准上，Uni-CoT在感知、概念和程序类别上超越所有开源基线，甚至在总分上超过闭源的Gemini 2.0模型5.59分。在RISE基准上，Uni-CoT与Gemini 2.0在四个推理类别和指令推理、外观一致性、视觉合理性的子维度评估指标上表现相当。
定性分析表明，Uni-CoT能够通过多轮自检纠正语义不准确的生成结果，对于需要常识推理的复杂提示，通过迭代重新评估和优化生成输出，确保最终视觉状态在多模态推理轨迹中实现更好的提示对齐和视觉连贯性。